#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM åº”ç”¨å¼€å‘å­¦ä¹  Demo
åŒ…å«å¤šç§ LLM åº”ç”¨åœºæ™¯çš„ç¤ºä¾‹
"""

import gradio as gr
import json
import re
from datetime import datetime
import random

class LLMSimulator:
    """LLM æ¨¡æ‹Ÿå™¨ - ç”¨äºæ¼”ç¤º LLM åº”ç”¨å¼€å‘"""
    
    def __init__(self):
        self.conversation_history = []
        self.system_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
    
    def chat_completion(self, messages, temperature=0.7, max_tokens=1000):
        """æ¨¡æ‹Ÿ LLM èŠå¤©å®Œæˆ"""
        # æ¨¡æ‹Ÿ LLM å“åº”
        user_message = messages[-1]["content"] if messages else ""
        
        # ç®€å•çš„è§„åˆ™åŸºç¡€å“åº”
        response = self._generate_response(user_message)
        
        return {
            "choices": [{
                "message": {
                    "content": response,
                    "role": "assistant"
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": len(user_message),
                "completion_tokens": len(response),
                "total_tokens": len(user_message) + len(response)
            }
        }
    
    def _generate_response(self, user_input):
        """ç”Ÿæˆæ¨¡æ‹Ÿå“åº”"""
        user_input_lower = user_input.lower()
        
        # é¢„è®¾çš„å“åº”æ¨¡å¼
        if "ä½ å¥½" in user_input or "hello" in user_input_lower:
            return "ä½ å¥½ï¼æˆ‘æ˜¯AIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"
        
        elif "å¤©æ°”" in user_input:
            return "ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæ¸©åº¦25Â°Cï¼Œé€‚åˆå¤–å‡ºæ´»åŠ¨ã€‚"
        
        elif "æ—¶é—´" in user_input:
            return f"ç°åœ¨æ˜¯ {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}"
        
        elif "è®¡ç®—" in user_input or "æ•°å­¦" in user_input:
            return "æˆ‘å¯ä»¥å¸®åŠ©æ‚¨è¿›è¡ŒåŸºæœ¬çš„æ•°å­¦è®¡ç®—ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨æƒ³è®¡ç®—ä»€ä¹ˆï¼Ÿ"
        
        elif "ç¿»è¯‘" in user_input:
            return "æˆ‘å¯ä»¥å¸®åŠ©æ‚¨ç¿»è¯‘æ–‡æœ¬ã€‚è¯·æä¾›éœ€è¦ç¿»è¯‘çš„å†…å®¹å’Œç›®æ ‡è¯­è¨€ã€‚"
        
        elif "æ€»ç»“" in user_input:
            return "æˆ‘å¯ä»¥å¸®åŠ©æ‚¨æ€»ç»“æ–‡æœ¬å†…å®¹ã€‚è¯·æä¾›éœ€è¦æ€»ç»“çš„æ–‡æœ¬ã€‚"
        
        elif "ä»£ç " in user_input or "ç¼–ç¨‹" in user_input:
            return "æˆ‘å¯ä»¥å¸®åŠ©æ‚¨ç¼–å†™å’Œè§£é‡Šä»£ç ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨éœ€è¦ä»€ä¹ˆç¼–ç¨‹å¸®åŠ©ï¼Ÿ"
        
        else:
            responses = [
                "è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰è¶£çš„é—®é¢˜ã€‚è®©æˆ‘æƒ³æƒ³...",
                "æˆ‘ç†è§£æ‚¨çš„é—®é¢˜ï¼Œè®©æˆ‘ä¸ºæ‚¨è¯¦ç»†è§£ç­”ã€‚",
                "æ ¹æ®æˆ‘çš„ç†è§£ï¼Œè¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆæ˜¯...",
                "è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ï¼Œè®©æˆ‘ä»å‡ ä¸ªè§’åº¦æ¥åˆ†æã€‚",
                "æˆ‘å»ºè®®æ‚¨å¯ä»¥è¿™æ ·è€ƒè™‘è¿™ä¸ªé—®é¢˜..."
            ]
            return random.choice(responses) + f"\n\nå…³äº'{user_input}'ï¼Œæˆ‘è®¤ä¸º..."

# åˆ›å»º LLM æ¨¡æ‹Ÿå™¨å®ä¾‹
llm_simulator = LLMSimulator()

def chat_with_llm(message, history, temperature, max_tokens):
    """ä¸ LLM èŠå¤©"""
    if not message.strip():
        return "", history
    
    # æ„å»ºæ¶ˆæ¯å†å²
    messages = [{"role": "system", "content": llm_simulator.system_prompt}]
    
    # æ·»åŠ å†å²å¯¹è¯
    for human, assistant in history:
        messages.append({"role": "user", "content": human})
        messages.append({"role": "assistant", "content": assistant})
    
    # æ·»åŠ å½“å‰æ¶ˆæ¯
    messages.append({"role": "user", "content": message})
    
    # è°ƒç”¨ LLM
    try:
        response = llm_simulator.chat_completion(
            messages, 
            temperature=temperature, 
            max_tokens=max_tokens
        )
        
        assistant_response = response["choices"][0]["message"]["content"]
        
        # æ˜¾ç¤ºä½¿ç”¨ç»Ÿè®¡
        usage_info = f"\n\n--- ä½¿ç”¨ç»Ÿè®¡ ---\n"
        usage_info += f"è¾“å…¥tokens: {response['usage']['prompt_tokens']}\n"
        usage_info += f"è¾“å‡ºtokens: {response['usage']['completion_tokens']}\n"
        usage_info += f"æ€»tokens: {response['usage']['total_tokens']}"
        
        return "", history + [[message, assistant_response + usage_info]]
    
    except Exception as e:
        return "", history + [[message, f"é”™è¯¯ï¼š{str(e)}"]]

def text_summarizer(text, max_length):
    """æ–‡æœ¬æ€»ç»“åŠŸèƒ½"""
    if not text.strip():
        return "è¯·è¾“å…¥è¦æ€»ç»“çš„æ–‡æœ¬"
    
    # ç®€å•çš„æ–‡æœ¬æ€»ç»“é€»è¾‘
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    if len(sentences) <= 3:
        return "æ–‡æœ¬å¤ªçŸ­ï¼Œæ— éœ€æ€»ç»“ã€‚åŸæ–‡ï¼š\n" + text
    
    # é€‰æ‹©å…³é”®å¥å­
    key_sentences = sentences[:max(2, len(sentences)//3)]
    summary = "ã€‚".join(key_sentences) + "ã€‚"
    
    return f"æ€»ç»“ç»“æœï¼š\n{summary}\n\nåŸæ–‡é•¿åº¦ï¼š{len(text)} å­—ç¬¦\næ€»ç»“é•¿åº¦ï¼š{len(summary)} å­—ç¬¦"

def code_explainer(code, language):
    """ä»£ç è§£é‡ŠåŠŸèƒ½"""
    if not code.strip():
        return "è¯·è¾“å…¥è¦è§£é‡Šçš„ä»£ç "
    
    # ç®€å•çš„ä»£ç åˆ†æ
    lines = code.split('\n')
    line_count = len(lines)
    char_count = len(code)
    
    # æ£€æµ‹ä»£ç ç‰¹å¾
    features = []
    if 'def ' in code:
        features.append("åŒ…å«å‡½æ•°å®šä¹‰")
    if 'class ' in code:
        features.append("åŒ…å«ç±»å®šä¹‰")
    if 'import ' in code or 'from ' in code:
        features.append("åŒ…å«å¯¼å…¥è¯­å¥")
    if 'if ' in code:
        features.append("åŒ…å«æ¡ä»¶è¯­å¥")
    if 'for ' in code or 'while ' in code:
        features.append("åŒ…å«å¾ªç¯è¯­å¥")
    
    explanation = f"""
ä»£ç åˆ†æç»“æœï¼š

ğŸ“Š åŸºæœ¬ä¿¡æ¯ï¼š
- ç¼–ç¨‹è¯­è¨€ï¼š{language}
- ä»£ç è¡Œæ•°ï¼š{line_count}
- å­—ç¬¦æ•°é‡ï¼š{char_count}

ğŸ” ä»£ç ç‰¹å¾ï¼š
{chr(10).join(f"- {feature}" for feature in features) if features else "- åŸºç¡€ä»£ç ç»“æ„"}

ğŸ’¡ å»ºè®®ï¼š
- ç¡®ä¿ä»£ç æœ‰é€‚å½“çš„æ³¨é‡Š
- éµå¾ª {language} çš„ç¼–ç è§„èŒƒ
- è€ƒè™‘ä»£ç çš„å¯è¯»æ€§å’Œç»´æŠ¤æ€§
"""
    
    return explanation

def prompt_engineering_demo(user_input, system_prompt, temperature):
    """æç¤ºå·¥ç¨‹æ¼”ç¤º"""
    if not user_input.strip():
        return "è¯·è¾“å…¥ç”¨æˆ·è¾“å…¥"
    
    # æ„å»ºä¸åŒçš„ç³»ç»Ÿæç¤º
    prompts = {
        "é»˜è®¤": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚",
        "ä¸“å®¶": "ä½ æ˜¯ä¸€ä¸ªé¢†åŸŸä¸“å®¶ï¼Œè¯·ç”¨ä¸“ä¸šæœ¯è¯­å›ç­”ã€‚",
        "æ•™å¸ˆ": "ä½ æ˜¯ä¸€ä¸ªè€å¿ƒçš„æ•™å¸ˆï¼Œè¯·ç”¨ç®€å•æ˜“æ‡‚çš„æ–¹å¼è§£é‡Šã€‚",
        "åˆ›æ„": "ä½ æ˜¯ä¸€ä¸ªåˆ›æ„åŠ©æ‰‹ï¼Œè¯·æä¾›åˆ›æ–°çš„æƒ³æ³•å’Œè§£å†³æ–¹æ¡ˆã€‚",
        "åˆ†æ": "ä½ æ˜¯ä¸€ä¸ªåˆ†æå¸ˆï¼Œè¯·æä¾›è¯¦ç»†çš„åˆ†æå’Œæ¨ç†è¿‡ç¨‹ã€‚"
    }
    
    results = []
    for prompt_type, prompt in prompts.items():
        # æ¨¡æ‹Ÿä¸åŒæç¤ºçš„æ•ˆæœ
        if prompt_type == "ä¸“å®¶":
            response = f"ä½œä¸º{prompt_type}ï¼Œæˆ‘çš„ä¸“ä¸šåˆ†ææ˜¯ï¼š{user_input}æ¶‰åŠå¤æ‚çš„ä¸“ä¸šæ¦‚å¿µ..."
        elif prompt_type == "æ•™å¸ˆ":
            response = f"è®©æˆ‘ç”¨ç®€å•çš„æ–¹å¼è§£é‡Šï¼š{user_input}å°±åƒ..."
        elif prompt_type == "åˆ›æ„":
            response = f"ä»åˆ›æ„è§’åº¦è€ƒè™‘ï¼š{user_input}å¯ä»¥è¿™æ ·åˆ›æ–°åœ°è§£å†³..."
        elif prompt_type == "åˆ†æ":
            response = f"è¯¦ç»†åˆ†æï¼šé¦–å…ˆï¼Œ{user_input}éœ€è¦è€ƒè™‘ä»¥ä¸‹å› ç´ ..."
        else:
            response = f"å…³äº{user_input}ï¼Œæˆ‘çš„å›ç­”æ˜¯..."
        
        results.append(f"**{prompt_type}æ¨¡å¼ï¼š**\n{prompt}\n\n**å›ç­”ï¼š**\n{response}\n")
    
    return "\n---\n".join(results)

def rag_demo(query, documents):
    """RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) æ¼”ç¤º"""
    if not query.strip():
        return "è¯·è¾“å…¥æŸ¥è¯¢é—®é¢˜"
    
    if not documents.strip():
        return "è¯·è¾“å…¥æ–‡æ¡£å†…å®¹"
    
    # ç®€å•çš„æ–‡æ¡£æ£€ç´¢
    doc_lines = documents.split('\n')
    relevant_docs = []
    
    # ç®€å•çš„å…³é”®è¯åŒ¹é…
    query_words = query.lower().split()
    for i, line in enumerate(doc_lines):
        if any(word in line.lower() for word in query_words):
            relevant_docs.append(f"æ–‡æ¡£ç¬¬{i+1}è¡Œï¼š{line}")
    
    if not relevant_docs:
        relevant_docs = ["æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£"]
    
    # ç”Ÿæˆå›ç­”
    context = "\n".join(relevant_docs[:3])  # å–å‰3ä¸ªç›¸å…³æ–‡æ¡£
    
    response = f"""
RAG æ£€ç´¢ç»“æœï¼š

ğŸ” æŸ¥è¯¢ï¼š{query}

ğŸ“„ ç›¸å…³æ–‡æ¡£ï¼š
{context}

ğŸ’¡ åŸºäºæ£€ç´¢å†…å®¹çš„å›ç­”ï¼š
æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œå…³äº"{query}"çš„ç­”æ¡ˆæ˜¯...
"""
    
    return response

def function_calling_demo(user_input):
    """å‡½æ•°è°ƒç”¨æ¼”ç¤º"""
    if not user_input.strip():
        return "è¯·è¾“å…¥ç”¨æˆ·è¾“å…¥"
    
    # æ¨¡æ‹Ÿå‡½æ•°è°ƒç”¨æ£€æµ‹
    functions = {
        "å¤©æ°”": {
            "name": "get_weather",
            "description": "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯",
            "parameters": {
                "city": {"type": "string", "description": "åŸå¸‚åç§°"}
            }
        },
        "è®¡ç®—": {
            "name": "calculate",
            "description": "æ‰§è¡Œæ•°å­¦è®¡ç®—",
            "parameters": {
                "expression": {"type": "string", "description": "æ•°å­¦è¡¨è¾¾å¼"}
            }
        },
        "ç¿»è¯‘": {
            "name": "translate",
            "description": "ç¿»è¯‘æ–‡æœ¬",
            "parameters": {
                "text": {"type": "string", "description": "è¦ç¿»è¯‘çš„æ–‡æœ¬"},
                "target_language": {"type": "string", "description": "ç›®æ ‡è¯­è¨€"}
            }
        }
    }
    
    detected_functions = []
    for keyword, func in functions.items():
        if keyword in user_input:
            detected_functions.append(func)
    
    if detected_functions:
        result = "æ£€æµ‹åˆ°å‡½æ•°è°ƒç”¨éœ€æ±‚ï¼š\n\n"
        for func in detected_functions:
            result += f"ğŸ“ å‡½æ•°ï¼š{func['name']}\n"
            result += f"ğŸ“ æè¿°ï¼š{func['description']}\n"
            result += f"ğŸ”§ å‚æ•°ï¼š{json.dumps(func['parameters'], ensure_ascii=False, indent=2)}\n\n"
    else:
        result = "æœªæ£€æµ‹åˆ°ç‰¹å®šçš„å‡½æ•°è°ƒç”¨éœ€æ±‚ã€‚"
    
    return result

# åˆ›å»º Gradio ç•Œé¢
with gr.Blocks(title="LLM åº”ç”¨å¼€å‘å­¦ä¹  Demo", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¤– LLM åº”ç”¨å¼€å‘å­¦ä¹  Demo")
    gr.Markdown("è¿™ä¸ªæ¼”ç¤ºåº”ç”¨å±•ç¤ºäº† LLM åº”ç”¨å¼€å‘ä¸­çš„å¸¸è§åœºæ™¯å’ŒæŠ€æœ¯ã€‚")
    
    with gr.Tab("ğŸ’¬ èŠå¤©å¯¹è¯"):
        gr.Markdown("### LLM èŠå¤©å¯¹è¯æ¼”ç¤º")
        with gr.Row():
            with gr.Column(scale=3):
                chatbot = gr.Chatbot(height=400, label="å¯¹è¯å†å²")
                msg = gr.Textbox(label="è¾“å…¥æ¶ˆæ¯", placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...")
                with gr.Row():
                    clear = gr.Button("æ¸…ç©ºå¯¹è¯")
                    submit = gr.Button("å‘é€", variant="primary")
            with gr.Column(scale=1):
                temperature = gr.Slider(0.1, 2.0, value=0.7, label="Temperature")
                max_tokens = gr.Slider(100, 2000, value=1000, step=100, label="Max Tokens")
        
        submit.click(chat_with_llm, [msg, chatbot, temperature, max_tokens], [msg, chatbot])
        msg.submit(chat_with_llm, [msg, chatbot, temperature, max_tokens], [msg, chatbot])
        clear.click(lambda: ([], ""), outputs=[chatbot, msg])
    
    with gr.Tab("ğŸ“ æ–‡æœ¬æ€»ç»“"):
        gr.Markdown("### æ–‡æœ¬æ€»ç»“åŠŸèƒ½æ¼”ç¤º")
        with gr.Row():
            with gr.Column():
                text_input = gr.Textbox(
                    label="è¾“å…¥æ–‡æœ¬",
                    placeholder="è¯·è¾“å…¥è¦æ€»ç»“çš„æ–‡æœ¬...",
                    lines=8
                )
                max_length = gr.Slider(50, 500, value=200, label="æ€»ç»“æœ€å¤§é•¿åº¦")
                summarize_btn = gr.Button("ç”Ÿæˆæ€»ç»“", variant="primary")
            with gr.Column():
                summary_output = gr.Textbox(label="æ€»ç»“ç»“æœ", lines=10)
        
        summarize_btn.click(text_summarizer, [text_input, max_length], summary_output)
    
    with gr.Tab("ğŸ’» ä»£ç è§£é‡Š"):
        gr.Markdown("### ä»£ç è§£é‡ŠåŠŸèƒ½æ¼”ç¤º")
        with gr.Row():
            with gr.Column():
                code_input = gr.Textbox(
                    label="è¾“å…¥ä»£ç ",
                    placeholder="è¯·è¾“å…¥è¦è§£é‡Šçš„ä»£ç ...",
                    lines=10
                )
                language = gr.Dropdown(
                    choices=["Python", "JavaScript", "Java", "C++", "Go"],
                    label="ç¼–ç¨‹è¯­è¨€",
                    value="Python"
                )
                explain_btn = gr.Button("è§£é‡Šä»£ç ", variant="primary")
            with gr.Column():
                explanation_output = gr.Textbox(label="ä»£ç è§£é‡Š", lines=12)
        
        explain_btn.click(code_explainer, [code_input, language], explanation_output)
    
    with gr.Tab("ğŸ¯ æç¤ºå·¥ç¨‹"):
        gr.Markdown("### æç¤ºå·¥ç¨‹æ¼”ç¤º")
        with gr.Row():
            with gr.Column():
                prompt_input = gr.Textbox(
                    label="ç”¨æˆ·è¾“å…¥",
                    placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                    lines=3
                )
                system_prompt = gr.Textbox(
                    label="ç³»ç»Ÿæç¤º",
                    value="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚",
                    lines=2
                )
                prompt_temperature = gr.Slider(0.1, 2.0, value=0.7, label="Temperature")
                prompt_btn = gr.Button("ç”Ÿæˆå›ç­”", variant="primary")
            with gr.Column():
                prompt_output = gr.Textbox(label="ä¸åŒæç¤ºçš„æ•ˆæœå¯¹æ¯”", lines=15)
        
        prompt_btn.click(prompt_engineering_demo, [prompt_input, system_prompt, prompt_temperature], prompt_output)
    
    with gr.Tab("ğŸ” RAG æ£€ç´¢"):
        gr.Markdown("### RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) æ¼”ç¤º")
        with gr.Row():
            with gr.Column():
                query_input = gr.Textbox(
                    label="æŸ¥è¯¢é—®é¢˜",
                    placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                    lines=2
                )
                documents_input = gr.Textbox(
                    label="æ–‡æ¡£å†…å®¹",
                    placeholder="è¯·è¾“å…¥æ–‡æ¡£å†…å®¹ï¼Œæ¯è¡Œä¸€ä¸ªæ–‡æ¡£ç‰‡æ®µ...",
                    lines=8
                )
                rag_btn = gr.Button("RAG æ£€ç´¢", variant="primary")
            with gr.Column():
                rag_output = gr.Textbox(label="RAG ç»“æœ", lines=12)
        
        rag_btn.click(rag_demo, [query_input, documents_input], rag_output)
    
    with gr.Tab("ğŸ“ å‡½æ•°è°ƒç”¨"):
        gr.Markdown("### å‡½æ•°è°ƒç”¨æ¼”ç¤º")
        with gr.Row():
            with gr.Column():
                func_input = gr.Textbox(
                    label="ç”¨æˆ·è¾“å…¥",
                    placeholder="ä¾‹å¦‚ï¼šæŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”ã€è®¡ç®— 2+3*4ã€ç¿»è¯‘ hello world",
                    lines=3
                )
                func_btn = gr.Button("æ£€æµ‹å‡½æ•°è°ƒç”¨", variant="primary")
            with gr.Column():
                func_output = gr.Textbox(label="å‡½æ•°è°ƒç”¨æ£€æµ‹ç»“æœ", lines=10)
        
        func_btn.click(function_calling_demo, func_input, func_output)
    
    with gr.Tab("ğŸ“š å­¦ä¹ èµ„æº"):
        gr.Markdown("""
        ### LLM åº”ç”¨å¼€å‘å­¦ä¹ èµ„æº
        
        #### ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ
        - **æç¤ºå·¥ç¨‹ (Prompt Engineering)**: è®¾è®¡æœ‰æ•ˆçš„æç¤ºæ¥è·å¾—æ›´å¥½çš„ LLM å“åº”
        - **RAG (Retrieval-Augmented Generation)**: ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æ··åˆæ–¹æ³•
        - **å‡½æ•°è°ƒç”¨ (Function Calling)**: è®© LLM èƒ½å¤Ÿè°ƒç”¨å¤–éƒ¨å‡½æ•°
        - **å¾®è°ƒ (Fine-tuning)**: é’ˆå¯¹ç‰¹å®šä»»åŠ¡ä¼˜åŒ–æ¨¡å‹
        
        #### ğŸ› ï¸ å¸¸ç”¨å·¥å…·
        - **OpenAI API**: æœ€æµè¡Œçš„ LLM API
        - **LangChain**: LLM åº”ç”¨å¼€å‘æ¡†æ¶
        - **Gradio**: å¿«é€Ÿæ„å»º Web ç•Œé¢
        - **Streamlit**: æ•°æ®ç§‘å­¦åº”ç”¨æ¡†æ¶
        
        #### ğŸ“– æ¨èå­¦ä¹ è·¯å¾„
        1. æŒæ¡åŸºç¡€ API è°ƒç”¨
        2. å­¦ä¹ æç¤ºå·¥ç¨‹æŠ€å·§
        3. å®è·µ RAG åº”ç”¨å¼€å‘
        4. æ¢ç´¢å‡½æ•°è°ƒç”¨åŠŸèƒ½
        5. æ„å»ºå®Œæ•´çš„åº”ç”¨
        
        #### ğŸ”— æœ‰ç”¨é“¾æ¥
        - [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs)
        - [LangChain æ–‡æ¡£](https://python.langchain.com/)
        - [Gradio æ–‡æ¡£](https://gradio.app/docs/)
        - [æç¤ºå·¥ç¨‹æŒ‡å—](https://www.promptingguide.ai/)
        
        **åˆ›å»ºæ—¶é—´ï¼š** """ + datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S"))

if __name__ == "__main__":
    # å¯åŠ¨åº”ç”¨
    demo.launch(
        server_name="0.0.0.0",
        server_port=7861,  # ä½¿ç”¨ä¸åŒç«¯å£é¿å…å†²çª
        share=False,
        debug=True
    ) 